{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing pour la classification de texte avec NLTK et Scikit-learn\n",
    "\n",
    "### Présenté par yassine NAJMI\n",
    "\n",
    "Dans le projet, Premiers pas avec le traitement du langage naturel en Python, nous avons appris les bases de la création de  tokenizing, part-of-speech tagging, stemming, chunking, Et named entity recognition; De plus, nous avons plongé dans l'apprentissage automatique et la classification de texte à l'aide d'un simple classificateur de vecteur de support et d'un ensemble de données de critiques positives et négatives.\n",
    "\n",
    "Dans ce Mini Projet, nous développerons cette base et explorerons différentes façons d'améliorer nos résultats de classification de texte. Nous couvrirons et utiliserons::\n",
    "\n",
    "* Expressions régulières\n",
    "* Ingénierie des fonctionnalités\n",
    "* Plusieurs classificateurs scikit-learn\n",
    "* Méthodes d'ensemble\n",
    "\n",
    "### 1. Importer les bibliothèques nécessaires\n",
    "\n",
    "Pour vous assurer que les bibliothèques nécessaires sont correctement installées et à jour, imprimez les numéros de version de chaque bibliothèque. Cela améliorera également la reproductibilité de notre projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 2.7.16 |Anaconda, Inc.| (default, Mar 14 2019, 15:42:17) [MSC v.1500 64 bit (AMD64)]\n",
      "NLTK: 3.4.5\n",
      "Scikit-learn: 0.20.3\n",
      "Pandas: 0.24.2\n",
      "Numpy: 1.16.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Charger le Dataset\n",
    "\n",
    "Maintenant que nous nous sommes assurés que nos bibliothèques sont installées correctement, nous allons charger l'ensemble de données en tant que Pandas dataframe. De plus, extrayons des informations utiles telles que les informations de colonne et les distributions de classe.\n",
    "\n",
    "L'ensemble de données que nous utiliserons provient du référentiel UCI Machine Learning. Il contient plus de 5000 messages SMS étiquetés qui ont été collectés pour la recherche de spam sur les téléphones mobiles. Il peut être téléchargé à partir de l'URL suivante:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# charger l'ensemble de données des messages SMS\n",
    "df = pd.read_table('DataSpamSms', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# imprimer des informations utiles sur l'ensemble de données\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# vérifier la distribution des classes\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prétraiter les données\n",
    "\n",
    "Le prétraitement des données est une étape essentielle du processus de langage naturel. Dans les cellules suivantes, nous convertirons nos étiquettes de classe en valeurs binaires à l'aide de LabelEncoder de sklearn, remplacerons les adresses e-mail, les URL, les numéros de téléphone et d'autres symboles à l'aide d'expressions régulières, supprimerons les mots vides et extraire les racines des mots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convertir les étiquettes de classe en valeurs binaires, 0 = ham and 1 = spam\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "5    FreeMsg Hey there darling it's been 3 week's n...\n",
      "6    Even my brother is not like to speak with me. ...\n",
      "7    As per your request 'Melle Melle (Oru Minnamin...\n",
      "8    WINNER!! As a valued network customer you have...\n",
      "9    Had your mobile 11 months or more? U R entitle...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# stocker les données du message SMS\n",
    "text_messages = df[1]\n",
    "print(text_messages[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Expressions régulières\n",
    "\n",
    "Some common regular expression metacharacters - copied from wikipedia\n",
    "\n",
    "**^**     Matches the starting position within the string. In line-based tools, it matches the starting position of any line.\n",
    "\n",
    "**.**     Matches any single character (many applications exclude newlines, and exactly which characters are considered newlines is flavor-, character-encoding-, and platform-specific, but it is safe to assume that the line feed character is included). Within POSIX bracket expressions, the dot character matches a literal dot. For example, a.c matches \"abc\", etc., but [a.c] matches only \"a\", \".\", or \"c\".\n",
    "\n",
    "**[ ]**    A bracket expression. Matches a single character that is contained within the brackets. For example, [abc] matches \"a\", \"b\", or \"c\". [a-z] specifies a range which matches any lowercase letter from \"a\" to \"z\". These forms can be mixed: [abcx-z] matches \"a\", \"b\", \"c\", \"x\", \"y\", or \"z\", as does [a-cx-z].\n",
    "The - character is treated as a literal character if it is the last or the first (after the ^, if present) character within the brackets: [abc-], [-abc]. Note that backslash escapes are not allowed. The ] character can be included in a bracket expression if it is the first (after the ^) character: []abc].\n",
    "\n",
    "**[^ ]**   Matches a single character that is not contained within the brackets. For example, [^abc] matches any character other than \"a\", \"b\", or \"c\". [^a-z] matches any single character that is not a lowercase letter from \"a\" to \"z\". Likewise, literal characters and ranges can be mixed.\n",
    "\n",
    "**$**      Matches the ending position of the string or the position just before a string-ending newline. In line-based tools, it matches the ending position of any line.\n",
    "\n",
    "**( )**    Defines a marked subexpression. The string matched within the parentheses can be recalled later (see the next entry, \\n). A marked subexpression is also called a block or capturing group. BRE mode requires \\( \\).\n",
    "\n",
    "**\\n**     Matches what the nth marked subexpression matched, where n is a digit from 1 to 9. This construct is vaguely defined in the POSIX.2 standard. Some tools allow referencing more than nine capturing groups.\n",
    "\n",
    "**\\***     Matches the preceding element zero or more times. For example, ab*c matches \"ac\", \"abc\", \"abbbc\", etc. [xyz]* matches \"\", \"x\", \"y\", \"z\", \"zx\", \"zyx\", \"xyzzy\", and so on. (ab)* matches \"\", \"ab\", \"abab\", \"ababab\", and so on.\n",
    "\n",
    "**{m,n}**  Matches the preceding element at least m and not more than n times. For example, a{3,5} matches only \"aaa\", \"aaaa\", and \"aaaaa\". This is not found in a few older instances of regexes. BRE mode requires \\{m,n\\}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser des expressions régulières pour remplacer les adresses e-mail, les URL, les numéros de téléphone et d'autres numéros\n",
    "\n",
    "# Remplacez les adresses e-mail par 'email'\n",
    "processed = text_messages.str.replace(r'^.+@[^\\.].*\\.[a-z]{2,}$',\n",
    "                                 'emailaddress')\n",
    "\n",
    "# Remplacer les URL par 'webaddress'\n",
    "processed = processed.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'webaddress')\n",
    "\n",
    "# Remplacez les symboles d'argent par 'moneysymb' \n",
    "processed = processed.str.replace(r'£|\\$', 'moneysymb')\n",
    "    \n",
    "# Remplacer les numéros de téléphone à 10 chiffres (les formats incluent les parenthèses, les espaces, aucun espace, les tirets) par 'phonenumber'\n",
    "processed = processed.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$',\n",
    "                                  'phonenumbr')\n",
    "    \n",
    "# Remplacer les nombres par 'numbr'\n",
    "processed = processed.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer la ponctuation\n",
    "processed = processed.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Remplacer les espaces entre les termes par un seul espace\n",
    "processed = processed.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Supprimer les espaces de début et de fin\n",
    "processed = processed.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       go jurong point crazi avail bugi n great world...\n",
      "1                                   ok lar joke wif u oni\n",
      "2       free entri numbr wkli comp win fa cup final tk...\n",
      "3                     u dun say earli hor u c alreadi say\n",
      "4                    nah think goe usf live around though\n",
      "5       freemsg hey darl numbr week word back like fun...\n",
      "6           even brother like speak treat like aid patent\n",
      "7       per request mell mell oru minnaminungint nurun...\n",
      "8       winner valu network custom select receivea num...\n",
      "9       mobil numbr month u r entitl updat latest colo...\n",
      "10      gonna home soon want talk stuff anymor tonight...\n",
      "11      six chanc win cash numbr numbr numbr pound txt...\n",
      "12      urgent numbr week free membership numbr numbr ...\n",
      "13      search right word thank breather promi wont ta...\n",
      "14                                            date sunday\n",
      "15      xxxmobilemovieclub use credit click wap link n...\n",
      "16                                             oh k watch\n",
      "17      eh u rememb numbr spell name ye v naughti make...\n",
      "18                             fine way u feel way gota b\n",
      "19      england v macedonia dont miss goal team news t...\n",
      "20                                      seriou spell name\n",
      "21                          go tri numbr month ha ha joke\n",
      "22                           pay first lar da stock comin\n",
      "23      aft finish lunch go str lor ard numbr smth lor...\n",
      "24                     ffffffffff alright way meet sooner\n",
      "25      forc eat slice realli hungri tho suck mark get...\n",
      "26                                      lol alway convinc\n",
      "27      catch bu fri egg make tea eat mom left dinner ...\n",
      "28                        back amp pack car let know room\n",
      "29                    ahhh work vagu rememb feel like lol\n",
      "                              ...                        \n",
      "5542                           armand say get ass epsilon\n",
      "5543                  u still havent got urself jacket ah\n",
      "5544    take derek amp taylor walmart back time done l...\n",
      "5545                               hi durban still number\n",
      "5546                               ic lotta childporn car\n",
      "5547    contract mobil numbr mnth latest motorola noki...\n",
      "5548                                        tri weekend v\n",
      "5549    know wot peopl wear shirt jumper hat belt know...\n",
      "5550                                  cool time think get\n",
      "5551                           wen get spiritu deep great\n",
      "5552    safe trip nigeria wish happi soon compani shar...\n",
      "5553                                hahaha use brain dear\n",
      "5554    well keep mind got enough ga one round trip ba...\n",
      "5555    yeh indian nice tho kane bit shud go numbr dri...\n",
      "5556                            ye u text pshew miss much\n",
      "5557    meant calcul lt gt unit lt gt school realli ex...\n",
      "5558                                     sorri call later\n",
      "5559                       next lt gt hour imma flip shit\n",
      "5560                                 anyth lor juz us lor\n",
      "5561                get dump heap mom decid come low bore\n",
      "5562    ok lor soni ericsson salesman ask shuhui say q...\n",
      "5563                               ard numbr like dat lor\n",
      "5564                     wait til least wednesday see get\n",
      "5565                                              huh lei\n",
      "5566    remind onumbr get numbr pound free call credit...\n",
      "5567    numbrnd time tri numbr contact u u numbr pound...\n",
      "5568                                b go esplanad fr home\n",
      "5569                                    piti mood suggest\n",
      "5570    guy bitch act like interest buy someth el next...\n",
      "5571                                       rofl true name\n",
      "Name: 1, Length: 5572, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# changer les mots en minuscules - Bonjour, BONJOUR, bonjour sont tous le même mot\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# supprimer les mots vides des messages texte\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retirer les tiges de mots à l'aide de Porter Stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Générer des fonctionnalités\n",
    "\n",
    "L'ingénierie des fonctionnalités consiste à utiliser la connaissance du domaine des données pour créer des fonctionnalités pour les algorithmes d'apprentissage automatique. Dans ce projet, les mots de chaque message texte seront nos fonctionnalités. Pour cela, il sera nécessaire de tokeniser chaque mot. Nous utiliserons les 1500 mots les plus courants comme fonctionnalités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 6562\n",
      "Most common words: [(u'numbr', 2961), (u'u', 1207), (u'call', 679), (u'go', 456), (u'get', 451), (u'ur', 391), (u'gt', 318), (u'lt', 316), (u'come', 304), (u'ok', 293), (u'free', 284), (u'day', 276), (u'know', 275), (u'love', 266), (u'like', 261)]\n"
     ]
    }
   ],
   "source": [
    "# imprimer le nombre total de mots et les 15 mots les plus communs\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utiliser les 1500 mots les plus communs comme caractéristiques\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avail\n",
      "buffet\n",
      "world\n",
      "great\n"
     ]
    }
   ],
   "source": [
    "# La fonction find_features déterminera les caractéristiques de 1500 mots contenues dans la revue\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Voyons un exemple!\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maintenant, nous allons le faire pour tous les messages\n",
    "messages = zip(processed, Y)\n",
    "\n",
    "# définir une graine pour la reproductibilité\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# appeler la fonction find_features pour chaque message SMS\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous pouvons diviser les ensembles de fonctionnalités en ensembles de données d'entraînement et de test à l'aide de sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# diviser les données en ensembles de données d'entraînement et de test\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4179\n",
      "1393\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classificateurs Scikit-Learn avec NLTK\n",
    "\n",
    "Maintenant que nous avons notre jeu de données, nous pouvons commencer à créer des algorithmes! Commençons par un simple classificateur de vecteur de support linéaire, puis développons d'autres algorithmes. Nous devrons importer chaque algorithme que nous prévoyons d'utiliser depuis sklearn. Nous devons également importer certaines mesures de performances, telles que precision_score et classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 95.6927494616\n"
     ]
    }
   ],
   "source": [
    "# Nous pouvons utiliser les algorithmes sklearn dans NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# faire Entrainer le modèle sur les données d'entraînement\n",
    "model.train(training)\n",
    "\n",
    "# et testez sur l'ensemble de données de test!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors Accuracy: 92.9648241206\n",
      "Decision Tree Accuracy: 94.7595118449\n",
      "Random Forest Accuracy: 95.1184493898\n",
      "Logistic Regression Accuracy: 95.4773869347\n",
      "SGD Classifier Accuracy: 95.8363244795\n",
      "Naive Bayes Accuracy: 95.2620244078\n",
      "SVM Linear Accuracy: 95.6927494616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Définir des modèles à former\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 94.4005743001\n"
     ]
    }
   ],
   "source": [
    "# Ensemble des méthodes - classificateur de vote\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faire une prédiction d'étiquette de classe pour l'ensemble de test\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1199\n",
      "           1       0.90      0.73      0.81       194\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      1393\n",
      "   macro avg       0.93      0.86      0.89      1393\n",
      "weighted avg       0.95      0.95      0.95      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1184</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>53</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1184   15\n",
       "       spam        53  141"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imprimer une matrice de confusion et un rapport de classification\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['ham', 'spam']],\n",
    "    columns = [['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merci Mon Prof Mr. Abdelhak Mahmoudi !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
